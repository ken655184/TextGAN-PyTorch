{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8264e4a42cfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0minstructor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstructor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBasicInstructor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeqGAN_D\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSeqGAN_D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeqGAN_G\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSeqGAN_G\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\TextGAN-PyTorch\\instructor\\oracle_data\\instructor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNLL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOracle\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOracle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGenDataIter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'metrics'"
     ]
    }
   ],
   "source": [
    "# %load seqgan_instructor.py\n",
    "# @Author       : William\n",
    "# @Project      : TextGAN-william\n",
    "# @FileName     : seqgan_instructor.py\n",
    "# @Time         : Created at 2019-04-25\n",
    "# @Blog         : http://zhiweil.ml/\n",
    "# @Description  : \n",
    "# Copyrights (C) 2018. All Rights Reserved.\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import config as cfg\n",
    "from instructor.oracle_data.instructor import BasicInstructor\n",
    "from models.SeqGAN_D import SeqGAN_D\n",
    "from models.SeqGAN_G import SeqGAN_G\n",
    "from utils import rollout\n",
    "from utils.data_loader import GenDataIter, DisDataIter\n",
    "\n",
    "\n",
    "class SeqGANInstructor(BasicInstructor):\n",
    "    def __init__(self, opt):\n",
    "        super(SeqGANInstructor, self).__init__(opt)\n",
    "\n",
    "        # generator, discriminator\n",
    "        self.gen = SeqGAN_G(cfg.gen_embed_dim, cfg.gen_hidden_dim, cfg.vocab_size, cfg.max_seq_len,\n",
    "                            cfg.padding_idx, gpu=cfg.CUDA)\n",
    "        self.dis = SeqGAN_D(cfg.dis_embed_dim, cfg.vocab_size, cfg.padding_idx, gpu=cfg.CUDA)\n",
    "        self.init_model()\n",
    "\n",
    "        # Optimizer\n",
    "        self.gen_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n",
    "        self.gen_adv_opt = optim.Adam(self.gen.parameters(), lr=cfg.gen_lr)\n",
    "        self.dis_opt = optim.Adam(self.dis.parameters(), lr=cfg.dis_lr)\n",
    "\n",
    "    def _run(self):\n",
    "        # ===PRE-TRAINING===\n",
    "        # TRAIN GENERATOR\n",
    "        if not cfg.gen_pretrain:\n",
    "            self.log.info('Starting Generator MLE Training...')\n",
    "            self.pretrain_generator(cfg.MLE_train_epoch)\n",
    "            if cfg.if_save and not cfg.if_test:\n",
    "                torch.save(self.gen.state_dict(), cfg.pretrained_gen_path)\n",
    "                print('Save pre-trained generator: {}'.format(cfg.pretrained_gen_path))\n",
    "\n",
    "        # ===TRAIN DISCRIMINATOR====\n",
    "        if not cfg.dis_pretrain:\n",
    "            self.log.info('Starting Discriminator Training...')\n",
    "            self.train_discriminator(cfg.d_step, cfg.d_epoch)\n",
    "            if cfg.if_save and not cfg.if_test:\n",
    "                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n",
    "                print('Save pre-trained discriminator: {}'.format(cfg.pretrained_dis_path))\n",
    "\n",
    "        # ===ADVERSARIAL TRAINING===\n",
    "        self.log.info('Starting Adversarial Training...')\n",
    "        self.log.info('Initial generator: %s' % (self.cal_metrics(fmt_str=True)))\n",
    "\n",
    "        for adv_epoch in range(cfg.ADV_train_epoch):\n",
    "            self.log.info('-----\\nADV EPOCH %d\\n-----' % adv_epoch)\n",
    "            self.sig.update()\n",
    "            if self.sig.adv_sig:\n",
    "                self.adv_train_generator(cfg.ADV_g_step)  # Generator\n",
    "                self.train_discriminator(cfg.ADV_d_step, cfg.ADV_d_epoch, 'ADV')  # Discriminator\n",
    "\n",
    "                if adv_epoch % cfg.adv_log_step == 0:\n",
    "                    if cfg.if_save and not cfg.if_test:\n",
    "                        self._save('ADV', adv_epoch)\n",
    "            else:\n",
    "                self.log.info('>>> Stop by adv_signal! Finishing adversarial training...')\n",
    "                break\n",
    "\n",
    "    def _test(self):\n",
    "        print('>>> Begin test...')\n",
    "\n",
    "        self._run()\n",
    "        pass\n",
    "\n",
    "    def pretrain_generator(self, epochs):\n",
    "        \"\"\"\n",
    "        Max Likelihood Pre-training for the generator\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            self.sig.update()\n",
    "            if self.sig.pre_sig:\n",
    "                pre_loss = self.train_gen_epoch(self.gen, self.oracle_data.loader, self.mle_criterion, self.gen_opt)\n",
    "\n",
    "                # ===Test===\n",
    "                if epoch % cfg.pre_log_step == 0 or epoch == epochs - 1:\n",
    "                    self.log.info(\n",
    "                        '[MLE-GEN] epoch %d : pre_loss = %.4f, %s' % (epoch, pre_loss, self.cal_metrics(fmt_str=True)))\n",
    "                    if cfg.if_save and not cfg.if_test:\n",
    "                        self._save('MLE', epoch)\n",
    "            else:\n",
    "                self.log.info('>>> Stop by pre signal, skip to adversarial training...')\n",
    "                break\n",
    "\n",
    "    def adv_train_generator(self, g_step):\n",
    "        \"\"\"\n",
    "        The gen is trained using policy gradients, using the reward from the discriminator.\n",
    "        Training is done for num_batches batches.\n",
    "        \"\"\"\n",
    "        rollout_func = rollout.ROLLOUT(self.gen, cfg.CUDA)\n",
    "        total_g_loss = 0\n",
    "        for step in range(g_step):\n",
    "            inp, target = GenDataIter.prepare(self.gen.sample(cfg.batch_size, cfg.batch_size), gpu=cfg.CUDA)\n",
    "\n",
    "            # ===Train===\n",
    "            rewards = rollout_func.get_reward(target, cfg.rollout_num, self.dis)\n",
    "            adv_loss = self.gen.batchPGLoss(inp, target, rewards)\n",
    "            self.optimize(self.gen_adv_opt, adv_loss)\n",
    "            total_g_loss += adv_loss.item()\n",
    "\n",
    "        # ===Test===\n",
    "        self.log.info('[ADV-GEN]: g_loss = %.4f, %s' % (total_g_loss, self.cal_metrics(fmt_str=True)))\n",
    "\n",
    "    def train_discriminator(self, d_step, d_epoch, phase='MLE'):\n",
    "        \"\"\"\n",
    "        Training the discriminator on real_data_samples (positive) and generated samples from gen (negative).\n",
    "        Samples are drawn d_step times, and the discriminator is trained for d_epoch d_epoch.\n",
    "        \"\"\"\n",
    "        # prepare loader for validate\n",
    "        global d_loss, train_acc\n",
    "        pos_val = self.oracle.sample(8 * cfg.batch_size, 4 * cfg.batch_size)\n",
    "        neg_val = self.gen.sample(8 * cfg.batch_size, 4 * cfg.batch_size)\n",
    "        dis_eval_data = DisDataIter(pos_val, neg_val)\n",
    "\n",
    "        for step in range(d_step):\n",
    "            # prepare loader for training\n",
    "            pos_samples = self.oracle_samples  # not re-sample the Oracle data\n",
    "            neg_samples = self.gen.sample(cfg.samples_num, 4 * cfg.batch_size)\n",
    "            dis_data = DisDataIter(pos_samples, neg_samples)\n",
    "\n",
    "            for epoch in range(d_epoch):\n",
    "                # ===Train===\n",
    "                d_loss, train_acc = self.train_dis_epoch(self.dis, dis_data.loader, self.dis_criterion,\n",
    "                                                         self.dis_opt)\n",
    "\n",
    "            # ===Test===\n",
    "            _, eval_acc = self.eval_dis(self.dis, dis_eval_data.loader, self.dis_criterion)\n",
    "            self.log.info('[%s-DIS] d_step %d: d_loss = %.4f, train_acc = %.4f, eval_acc = %.4f,' % (\n",
    "                phase, step, d_loss, train_acc, eval_acc))\n",
    "\n",
    "            if cfg.if_save and not cfg.if_test:\n",
    "                torch.save(self.dis.state_dict(), cfg.pretrained_dis_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
